{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "변수선택법이 적용되는 모델\n",
    "\n",
    "회귀 모델(Regression Models):\n",
    "선형 회귀(Linear Regression)\n",
    "로지스틱 회귀(Logistic Regression)\n",
    "릿지 회귀(Ridge Regression)\n",
    "라쏘 회귀(Lasso Regression)\n",
    "회귀 모델에서 변수 선택법은 모델의 설명력을 높이고, 불필요한 변수를 제거해 모델의 해석 가능성을 높이는 데 유용합니다.\n",
    "\n",
    "분류 모델(Classification Models):\n",
    "의사결정나무(Decision Trees)\n",
    "랜덤 포레스트(Random Forest)\n",
    "서포트 벡터 머신(Support Vector Machines, SVM)\n",
    "k-최근접 이웃(k-Nearest Neighbors, k-NN)\n",
    "분류 모델에서도 변수 선택은 중요한 역할을 하며, 특히 고차원 데이터셋에서 과적합을 방지하는 데 효과적입니다.\n",
    "클러스터링(Clustering) 및 차원 축소(Dimensionality Reduction):\n",
    "\n",
    "K-평균(K-Means)\n",
    "주성분 분석(Principal Component Analysis, PCA)\n",
    "t-SNE\n",
    "차원 축소 기법은 변수 선택법의 일종으로, 데이터의 차원을 줄이면서도 중요한 정보를 유지하는 데 중점을 둡니다.\n",
    "\n",
    "앙상블 모델(Ensemble Models):\n",
    "랜덤 포레스트(Random Forest)\n",
    "그래디언트 부스팅(Gradient Boosting)\n",
    "앙상블 모델에서는 변수 중요도를 기반으로 한 변수 선택이 자주 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변수선택법 유형\n",
    "\n",
    "# 필터 방법(Filter Methods):\n",
    "분산 임계값(Variance Threshold): 일정 수준 이하의 분산을 가진 변수를 제거합니다.\n",
    "상관계수(Correlation Coefficient): 종속 변수와의 상관관계에 따라 변수를 선택합니다.\n",
    "카이 제곱 검정(Chi-Square Test): 카이 제곱 통계를 이용해 독립성과 연관성이 높은 변수를 선택합니다.\n",
    "\n",
    "# 랩퍼 방법(Wrapper Methods):\n",
    "전진 선택법(Forward Selection), 후진 제거법(Backward Elimination): 반복적으로 변수를 추가하거나 제거하여 모델의 성능을 최적화합니다.\n",
    "RFE(Recursive Feature Elimination): 모델을 반복적으로 학습시켜 가장 덜 중요한 변수를 제거하는 방식입니다.\n",
    "\n",
    "# 임베디드 방법(Embedded Methods):\n",
    "라쏘(Lasso) 및 릿지(Ridge) 회귀: 페널티를 사용해 불필요한 변수를 자연스럽게 제거합니다.\n",
    "트리 기반 방법(Tree-based Methods): 의사결정나무, 랜덤 포레스트 등의 모델에서 변수 중요도를 계산해 변수를 선택합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변수선택방법 (필터방법)\n",
    "#\n",
    "분산 임계값\n",
    "\n",
    "# module import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "# VarianceThreshold 객체 생성 (임계값을 0.1로 설정)\n",
    "selector = VarianceThreshold(threshold=0.1)\n",
    "X_reduced = selector.fit_transform(df)\n",
    "\n",
    "# 선택된 변수의 인덱스 확인\n",
    "selected_features = df.columns[selector.get_support()]\n",
    "\n",
    "print(\"\\nSelected features:\")\n",
    "print(selected_features)\n",
    "\n",
    "# 변환된 데이터 출력\n",
    "print(\"\\nReduced data:\")\n",
    "print(X_reduced)\n",
    "\n",
    "# 참고사항\n",
    "임계값 선택: 임계값을 너무 낮게 설정하면 중요한 변수가 제거될 수 있습니다. 반대로, 너무 높게 설정하면 불필요한 변수들이 남을 수 있습니다.\n",
    "데이터의 스케일: 변수가 스케일이 다른 경우, 데이터 표준화(Standardization)를 수행한 후 분산 임계값을 적용하는 것이 좋습니다.\n",
    "\n",
    "#\n",
    "상관계수 \n",
    "\n",
    "# module import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 보스턴 주택 가격 데이터 로드\n",
    "data = load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='PRICE')\n",
    "\n",
    "# 데이터프레임으로 결합\n",
    "df = pd.concat([X, y], axis=1)\n",
    "\n",
    "# 각 독립 변수와 종속 변수 간의 상관계수 계산\n",
    "correlation_matrix = df.corr()\n",
    "correlation_with_target = correlation_matrix['PRICE'].drop('PRICE')\n",
    "\n",
    "print(\"Correlation with target variable (PRICE):\")\n",
    "print(correlation_with_target)\n",
    "\n",
    "# 상관계수가 높은 변수 선택 (임계값 설정: 0.5)\n",
    "selected_features = correlation_with_target[abs(correlation_with_target) > 0.5].index\n",
    "\n",
    "print(\"\\nSelected features based on correlation:\")\n",
    "print(selected_features)\n",
    "\n",
    "#\n",
    "카이제곱검정\n",
    "\n",
    "# module import\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Iris 데이터셋 로드\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 데이터 표준화 (카이제곱 검정은 비율에 영향을 받을 수 있으므로 표준화가 필요)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 카이제곱 검정을 사용하여 특성 선택\n",
    "selector = SelectKBest(score_func=chi2, k='all')  # 모든 특성에 대해 검정\n",
    "selector.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 각 특성의 카이제곱 통계량과 p-value 출력\n",
    "scores = selector.scores_\n",
    "p_values = selector.pvalues_\n",
    "\n",
    "features = X.columns\n",
    "results = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Chi2 Score': scores,\n",
    "    'p-value': p_values\n",
    "})\n",
    "\n",
    "print(results)\n",
    "\n",
    "# k 최적 값으로 특정 특성만 선택할 수 있습니다. 예를 들어, 가장 중요한 2개의 특성 선택\n",
    "selector = SelectKBest(score_func=chi2, k=2)\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "# 선택된 특성의 인덱스\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "\n",
    "print(\"\\nSelected features:\")\n",
    "print(selected_features)\n",
    "\n",
    "# 주의사항\n",
    "카이제곱 검정의 전제 조건: 카이제곱 검정은 특성이 범주형 데이터일 때 가장 적합합니다. 연속형 데이터는 카이제곱 검정을 직접 적용하기 어렵습니다. 이 예제에서는 연속형 데이터를 사용하므로, 실제 적용 시에는 적절한 전처리와 변환이 필요할 수 있습니다.\n",
    "데이터의 스케일: 카이제곱 검정은 데이터의 스케일에 영향을 받을 수 있으므로, 특성 스케일링이 중요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변수선택방법 (랩퍼방법)\n",
    "\n",
    "#\n",
    "전진선택법\n",
    "\n",
    "# module import\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 데이터 로드\n",
    "data = load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='PRICE')\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 회귀 모델 생성\n",
    "model = LinearRegression()\n",
    "\n",
    "# 전진 선택법을 사용하여 변수 선택\n",
    "selector = SequentialFeatureSelector(model, n_features_to_select='auto', direction='forward', scoring='neg_mean_squared_error', cv=5)\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# 선택된 변수들\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "\n",
    "print(\"Selected features:\")\n",
    "print(selected_features)\n",
    "\n",
    "# 모델을 학습하고 성능 평가\n",
    "model.fit(X_train[selected_features], y_train)\n",
    "y_pred = model.predict(X_test[selected_features])\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"\\nMean Squared Error with selected features:\")\n",
    "print(mse)\n",
    "\n",
    "# 참고사항\n",
    "중지 기준: SequentialFeatureSelector는 내부적으로 교차 검증을 통해 모델의 성능을 평가하고, 변수 추가 시 성능 향상이 없을 때 중지합니다.\n",
    "모델 성능 평가: scoring 파라미터를 통해 모델 성능 평가 지표를 설정할 수 있습니다. 회귀 문제에서는 'neg_mean_squared_error'와 같은 지표를 사용할 수 있습니다.\n",
    "교차 검증: cv 파라미터를 통해 교차 검증의 folds 수를 설정할 수 있습니다.\n",
    "\n",
    "전진 선택법은 변수 선택을 자동화하고, 모델의 성능을 최적화하는 데 유용한 방법입니다. 하지만, 변수 선택의 결과는 데이터와 모델에 따라 달라질 수 있으므로, 다른 변수 선택 기법과 함께 사용하여 최적의 결과를 얻는 것이 좋습니다.\n",
    "\n",
    "#\n",
    " 후진제거법\n",
    " # module import\n",
    " import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터 로드\n",
    "data = load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='PRICE')\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 후진 제거법 함수 정의\n",
    "def backward_elimination(X, y, significance_level=0.05):\n",
    "    \"\"\"Perform backward elimination for feature selection.\"\"\"\n",
    "    features = X.columns\n",
    "    while len(features) > 0:\n",
    "        # 모델 학습\n",
    "        X_with_const = sm.add_constant(X[features])  # 상수항 추가\n",
    "        model = sm.OLS(y, X_with_const).fit()\n",
    "        \n",
    "        # p-value가 가장 높은 변수 확인\n",
    "        p_values = model.pvalues\n",
    "        max_p_value = p_values.max()\n",
    "        if max_p_value > significance_level:\n",
    "            # p-value가 임계값보다 크면 해당 변수 제거\n",
    "            feature_to_remove = p_values.idxmax()\n",
    "            features = features.drop(feature_to_remove)\n",
    "        else:\n",
    "            # 모든 변수의 p-value가 임계값 이하이면 종료\n",
    "            break\n",
    "\n",
    "    return features\n",
    "\n",
    "# 후진 제거법 수행\n",
    "selected_features = backward_elimination(X_train, y_train)\n",
    "\n",
    "print(\"Selected features after backward elimination:\")\n",
    "print(selected_features)\n",
    "\n",
    "# 선택된 특성으로 모델 학습 및 평가\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# 최종 모델 학습\n",
    "model = sm.OLS(y_train, sm.add_constant(X_train_selected)).fit()\n",
    "y_pred = model.predict(sm.add_constant(X_test_selected))\n",
    "\n",
    "# 성능 평가\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"\\nMean Squared Error with selected features:\")\n",
    "print(mse)\n",
    "\n",
    "# 참고사항\n",
    "p-value: p-value는 변수의 중요성을 나타내는 지표로, 보통 0.05 이하일 때 해당 변수가 통계적으로 유의미하다고 간주합니다.\n",
    "중지 기준: 후진 제거법은 p-value 외에도 다른 기준을 사용할 수 있습니다 (예: AIC, BIC 등).\n",
    "성능 평가: 모델의 성능을 평가하여 최종 모델의 성능이 개선되었는지 확인합니다.\n",
    "후진 제거법은 모든 변수를 포함한 상태에서 시작하여 단계적으로 변수들을 제거하는 방법으로, 모델의 성능을 최적화하는 데 유용합니다. 데이터와 모델의 특성에 따라 최적의 변수를 선택하는 과정에서 유연하게 적용할 수 있습니다.\n",
    "\n",
    "# \n",
    "RFE\n",
    "# module import \n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 보스턴 주택 가격 데이터 로드\n",
    "data = load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='PRICE')\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 회귀 모델 생성\n",
    "model = LinearRegression()\n",
    "\n",
    "# RFE를 사용한 변수 선택\n",
    "selector = RFE(estimator=model, n_features_to_select=5)  # 선택할 변수 개수 설정\n",
    "selector = selector.fit(X_train, y_train)\n",
    "\n",
    "# 선택된 변수들\n",
    "selected_features = X.columns[selector.support_]\n",
    "\n",
    "print(\"Selected features:\")\n",
    "print(selected_features)\n",
    "\n",
    "# 선택된 특성으로 모델 학습 및 평가\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# 최종 모델 학습\n",
    "model.fit(X_train_selected, y_train)\n",
    "y_pred = model.predict(X_test_selected)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"\\nMean Squared Error with selected features:\")\n",
    "print(mse)\n",
    "\n",
    "# 참고사항\n",
    "변수 개수 설정: n_features_to_select를 통해 선택할 변수의 개수를 설정할 수 있습니다. 이 값을 조정하여 최적의 변수 개수를 찾는 것이 좋습니다.\n",
    "다양한 모델: RFE는 회귀뿐만 아니라 분류 모델에도 적용할 수 있습니다. estimator 파라미터를 변경하여 다른 모델을 사용할 수 있습니다.\n",
    "변수 중요도 평가: RFE는 모델의 변수 중요도를 기반으로 변수를 선택합니다. 따라서, 모델의 성능이 중요한 요소가 됩니다.\n",
    "RFE는 변수 선택을 자동화하고, 모델의 성능을 최적화하는 데 유용한 방법입니다. 데이터의 특성에 맞게 변수 선택 과정을 조정하고, 다양한 모델을 실험하여 최적의 성능을 얻는 것이 좋습니다.\n",
    "\n",
    "#\n",
    "엘라스틱넷\n",
    "\n",
    "# module import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 데이터 로드\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='target')\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 엘라스틱넷 모델 생성 및 학습\n",
    "# ElasticNetCV는 교차 검증을 통해 최적의 α와 l1_ratio(ρ)를 찾습니다.\n",
    "model = ElasticNetCV(cv=5, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 선택된 변수의 가중치 확인\n",
    "selected_features = X.columns[model.coef_ != 0]\n",
    "print(\"Selected features using ElasticNet:\")\n",
    "print(selected_features)\n",
    "\n",
    "# 선택된 변수들로 모델 재학습 및 성능 평가\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "model.fit(X_train_selected, y_train)\n",
    "y_pred = model.predict(X_test_selected)\n",
    "\n",
    "# 모델 성능 평가 (예: MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"\\nMean Squared Error with selected features:\")\n",
    "print(mse)\n",
    "\n",
    "# colusion\n",
    "최종 결론\n",
    "엘라스틱넷은 회귀 모델에서 변수 선택을 효과적으로 수행할 수 있는 강력한 방법입니다. 이 방법은 특히 다중공선성이 있는 데이터나 고차원 데이터에서 유용하며, 선택된 변수들로 모델을 더 간결하고 해석 가능하게 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변수선택법 (임베디드방법)\n",
    "\n",
    "#\n",
    "라쏘 회귀\n",
    "# module import\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# 보스턴 주택 가격 데이터 로드\n",
    "data = load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='PRICE')\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 라쏘 회귀 모델 생성\n",
    "lasso = Lasso(alpha=0.1)  # alpha는 정규화 강도\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# 중요 변수 추출\n",
    "model = SelectFromModel(lasso, prefit=True)\n",
    "X_train_selected = model.transform(X_train)\n",
    "X_test_selected = model.transform(X_test)\n",
    "\n",
    "# 오류 발생 시 \n",
    "lasso_selected = Lasso(alpha=0.1)\n",
    "lasso_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# 선택된 변수들\n",
    "selected_features = X.columns[model.get_support()]\n",
    "\n",
    "print(\"Selected features using Lasso regression:\")\n",
    "print(selected_features)\n",
    "\n",
    "# 모델 성능 평가\n",
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = lasso.predict(X_test_selected)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"\\nMean Squared Error with selected features:\")\n",
    "print(mse)\n",
    "\n",
    "#\n",
    "릿지 회귀\n",
    "# module import\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# 보스턴 주택 가격 데이터 로드\n",
    "data = load_boston()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='PRICE')\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 릿지 회귀 모델 생성\n",
    "ridge = Ridge(alpha=1.0)  # alpha는 정규화 강도\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# 계수의 크기로 변수 중요도 평가\n",
    "coefficients = pd.Series(ridge.coef_, index=X.columns)\n",
    "\n",
    "# 중요 변수 추출 (계수의 절대값 기준)\n",
    "important_features = coefficients[abs(coefficients) > 0.1].index\n",
    "\n",
    "print(\"Important features using Ridge regression:\")\n",
    "print(important_features)\n",
    "\n",
    "# 모델 성능 평가\n",
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = ridge.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"\\nMean Squared Error with selected features:\")\n",
    "print(mse)\n",
    "# colusion\n",
    "라쏘 회귀: L1 정규화를 통해 변수 선택을 자연스럽게 수행하며, 중요하지 않은 변수의 계수를 0으로 만들어 변수 선택을 구현합니다.\n",
    "릿지 회귀: L2 정규화를 통해 모든 변수의 계수를 균등하게 줄이지만, 변수 선택보다는 과적합 방지에 중점을 둡니다. 변수의 중요도는 계수의 크기를 통해 간접적으로 평가할 수 있습니다.\n",
    "\n",
    "#\n",
    "트리기반방법\n",
    "\n",
    "#\n",
    "랜덤포레스트 기반\n",
    "# module import\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 데이터 로드\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='target')\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 랜덤 포레스트 모델 생성\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 변수 중요도 추출\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# 중요도 기준으로 변수 선택\n",
    "selector = SelectFromModel(model, threshold='mean', prefit=True)\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# 선택된 변수들\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "\n",
    "print(\"Selected features using Random Forest:\")\n",
    "print(selected_features)\n",
    "\n",
    "# 선택된 변수들로 모델 재학습 및 성능 평가\n",
    "model.fit(X_train_selected, y_train)\n",
    "y_pred = model.predict(X_test_selected)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nAccuracy with selected features:\")\n",
    "print(accuracy)\n",
    "\n",
    "#\n",
    "그라디언트 부스팅 기반\n",
    "# module import\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 데이터 로드\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='target')\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 그라디언트 부스팅 모델 생성\n",
    "model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 변수 중요도 추출\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# 중요도 기준으로 변수 선택\n",
    "selector = SelectFromModel(model, threshold='mean', prefit=True)\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# 선택된 변수들\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "\n",
    "print(\"Selected features using Gradient Boosting:\")\n",
    "print(selected_features)\n",
    "\n",
    "# 선택된 변수들로 모델 재학습 및 성능 평가\n",
    "model.fit(X_train_selected, y_train)\n",
    "y_pred = model.predict(X_test_selected)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nAccuracy with selected features:\")\n",
    "print(accuracy)\n",
    "\n",
    "# 참고사항\n",
    "변수 중요도 기준: SelectFromModel에서 threshold 파라미터를 조정하여 중요도가 높은 변수들만 선택할 수 있습니다. 예를 들어, threshold=0.01로 설정하면 중요도가 0.01 이상인 변수만 선택됩니다.\n",
    "하이퍼파라미터 튜닝: 그라디언트 부스팅 모델은 n_estimators, learning_rate, max_depth 등 여러 하이퍼파라미터가 있으며, 이들을 적절히 튜닝하여 최적의 성능을 얻는 것이 중요합니다.\n",
    "과적합 방지: 그라디언트 부스팅은 매우 강력한 모델이지만, 과적합의 위험이 있으므로 교차 검증을 통해 모델 성능을 평가하고 변수 선택의 효과를 검증하는 것이 좋습니다.\n",
    "\n",
    "\n",
    "# \n",
    "XGboost\n",
    "# module import \n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 데이터 로드\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='target')\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# XGBoost 모델 생성\n",
    "model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 변수 중요도 추출\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# 중요도 기준으로 변수 선택\n",
    "selector = SelectFromModel(model, threshold=\"mean\", prefit=True)\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# 선택된 변수들\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "\n",
    "print(\"Selected features using XGBoost:\")\n",
    "print(selected_features)\n",
    "\n",
    "# 선택된 변수들로 모델 재학습 및 성능 평가\n",
    "model.fit(X_train_selected, y_train)\n",
    "y_pred = model.predict(X_test_selected)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nAccuracy with selected features:\")\n",
    "print(accuracy)\n",
    "\n",
    "# 참고사항\n",
    "하이퍼파라미터 조정: XGBoost는 다양한 하이퍼파라미터를 제공합니다. learning_rate, max_depth, subsample 등의 파라미터를 조정하여 모델 성능을 최적화할 수 있습니다.\n",
    "변수 중요도 기준: threshold 파라미터를 조정하여 선택할 변수의 기준을 설정할 수 있습니다. 예를 들어, '0.01*mean'으로 설정하면 평균의 1% 이상의 중요도를 가진 변수만 선택됩니다.\n",
    "다양한 중요도 측정 방법: SelectFromModel을 사용할 때 기본적으로 중요도 기준은 gain을 사용합니다. importance_type 파라미터를 설정하여 다른 중요도 측정 방법(weight, cover)을 사용할 수 있습니다.\n",
    "XGBoost를 사용한 변수 선택은 특히 고차원 데이터에서 모델의 성능을 최적화하고 불필요한 변수를 제거하는 데 매우 유용합니다. 이 방법을 통해 모델의 복잡성을 줄이고, 해석 가능성을 높일 수 있습니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스케일링\n",
    "\n",
    "# module import \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "\n",
    "# 데이터 간단히 확인\n",
    "print(data.head())\n",
    "\n",
    "# 필요한 특성만 추출하고 결측치 처리 (간단한 예시)\n",
    "features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "X = data[features]\n",
    "y = data['Survived']\n",
    "\n",
    "# 결측치 처리 (간단히 평균으로 대체)\n",
    "X['Age'].fillna(X['Age'].mean(), inplace=True)\n",
    "\n",
    "# 특성과 레이블을 훈련 세트와 테스트 세트로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 표준화 (Standardization)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Standardized training data:\")\n",
    "print(X_train_scaled[:5])\n",
    "\n",
    "# 최소-최대 정규화 (Min-Max Scaling)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Min-Max scaled training data:\")\n",
    "print(X_train_scaled[:5])\n",
    "\n",
    "# 최대 절대값 스케일링 (MaxAbs Scaling)\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"MaxAbs scaled training data:\")\n",
    "print(X_train_scaled[:5])\n",
    "\n",
    "# 로버스트 스케일링 (Robust Scaling)\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Robust scaled training data:\")\n",
    "print(X_train_scaled[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "회귀분석\n",
    "# 향후 버전 업에 대한 경고 메시지 출력 안하기 \n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore') \n",
    "\n",
    "# module import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# data load\n",
    "boston_df = pd.read_csv('./Boston.csv')\n",
    "boston_df\n",
    "\n",
    "# data columns rename\n",
    "boston_df = boston_df[['crim','zn','indus','chas','nox','rm','age','dis','rad','tax','ptratio','black','lstat','medv']]\n",
    "boston_df.rename(columns={boston_df.columns[-1]:'price'},inplace=True)\n",
    "\n",
    "# 종속변수, 독립변수 설정 and 상관계수 생성\n",
    "cols = [\"price\", \"rm\", \"age\", \"rad\"]\n",
    "boston_df_corr = boston_df.corr()\n",
    "\n",
    "# 히트맵\n",
    "plt.figure(figsize=(15,15))\n",
    "mask = np.array(boston_df_corr)\n",
    "mask[np.tril_indices_from(mask)] = False\n",
    "sns.heatmap(boston_df_corr, mask = mask, annot=True, cmap='Blues')\n",
    "plt.show()\n",
    "\n",
    "# pairplot\n",
    "sns.pairplot(boston_df[cols], kind='reg')\n",
    "plt.show()\n",
    "\n",
    "# 데이터분할 module import\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X, Y 분할하기\n",
    "y = boston_df['PRICE']\n",
    "X = boston_df.drop(['PRICE'], axis=1, inplace=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# 회귀모델 모듈 load\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# model 생성\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 계수 확인\n",
    "model.coef_\n",
    "model.intercept_\n",
    "\n",
    "# 평균제곱오차, r2-score module load\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 예측모델 생성\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 평균제곱오차, r2-score output\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "\n",
    "#\n",
    "릿지회귀(L2)\n",
    "\n",
    "# module import\n",
    "sklearn.linear_model import Ridge\n",
    "\n",
    "# model\n",
    "model = Ridge(alpha = .1)\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "rmse\n",
    "r2\n",
    "\n",
    "#\n",
    "라쏘회귀(L1)\n",
    "\n",
    "# module import\n",
    "sklearn.linear_model from Lasso\n",
    "\n",
    "# model\n",
    "model =Lasso(alpha = .1)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "rmse\n",
    "r2\n",
    "\n",
    "# \n",
    "엘라스틱넷 \n",
    "\n",
    "# module import \n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# model\n",
    "model = ElasticNet(alpha = .1, l1_ratio = .5) \n",
    "-- l1_ratio = 0 [Ridge] l1_ratio = 1 [Lidge] range 0 ~ 1\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "rmse\n",
    "r2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로지스틱회귀 기본가정 검정\n",
    "# 선형성 검정 (Box_Tidwell test)\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X_train_const = sm.add_constant(X_train)\n",
    "logit_model = sm.Logit(y_train,X_train_const).fit()\n",
    "print(logit_model.summary())\n",
    "\n",
    "# 독립성 검정 (sm.Logit 사용시)\n",
    "residual = logit_model.resid_pearson\n",
    "\n",
    "plt.plot(residual, marker = 'o', linestyle = 'none', color = 'g', alpha = .5)\n",
    "plt.title('Residuals vs. Observation Index')\n",
    "plt.xlabel('Observation Index')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n",
    "# 이상치 및 영향력 있는 관측치 \n",
    "def compute_influence(logit_model, X):\n",
    "    \"\"\" Compute influence measures for Logit model \"\"\"\n",
    "    # 모델의 예측 확률\n",
    "    pred_probs = logit_model.predict(X)\n",
    "    \n",
    "    # 잔차 계산 (Pearson residuals)\n",
    "    residuals = (y_train - pred_probs) / np.sqrt(pred_probs * (1 - pred_probs))\n",
    "    \n",
    "    # Leverage 계산\n",
    "    H = np.dot(X, np.linalg.pinv(np.dot(X.T, X))).dot(X.T)\n",
    "    leverage = np.diag(H)\n",
    "    \n",
    "    # Cook's Distance 계산\n",
    "    influence = np.sqrt((residuals**2) * leverage / (1 - leverage))\n",
    "    \n",
    "    return leverage, influence\n",
    "\n",
    "leverage, cooks_distance = compute_influence(logit_model, X_train_const)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(range(len(leverage)), leverage, alpha=0.7)\n",
    "plt.title('Leverage')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Leverage')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(range(len(cooks_distance)), cooks_distance, alpha=0.7)\n",
    "plt.title(\"Cook's Distance\")\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel(\"Cook's Distance\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 다중공선성 확인 \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif['Variable'] = X_train_const.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train_const.values, i) for i in range(X_train_const.shape[1])]\n",
    "\n",
    "print(vif)\n",
    "\n",
    "# 모델 적합도 검정\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "y_pred_prob = logit_model.predict(X_train_const)\n",
    "roc_auc = roc_auc_score(y_train, y_pred_prob)\n",
    "fpr, tpr, _ = roc_curve(y_train, y_pred_prob)\n",
    "\n",
    "plt.plot(fpr, tpr, color = 'g')\n",
    "plt.fill_between(fpr, tpr, color='g', alpha=0.4)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분류 머신러닝 함수 (교차검증 + 그리드서치)\n",
    "# 분류모델 함수 (모두 적용)\n",
    "\n",
    "    # Evaluate model (common for train, val, test)\n",
    "    def evaluate_model(best_model, X, y):\n",
    "        y_pred = best_model.predict(X)\n",
    "\n",
    "        if hasattr(best_model, \"predict_proba\"):\n",
    "            y_prob = best_model.predict_proba(X)[:, 1]\n",
    "        elif hasattr(best_model, \"decision_function\"):\n",
    "            y_prob = best_model.decision_function(X)\n",
    "        else:\n",
    "            raise ValueError(\"Model does not support probability or decision function.\")\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y, y_prob, pos_label=1)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(y, y_prob, pos_label=1)\n",
    "        pr_auc = auc(recall, precision)\n",
    "\n",
    "        F1 = f1_score(y, y_pred)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "\n",
    "        report = classification_report(y, y_pred, output_dict=True)\n",
    "\n",
    "        return fpr, tpr, roc_auc, precision, recall, pr_auc, F1, accuracy, report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "회귀 머신러닝 함수 (교차검증 + 그리드서치)\n",
    "# 회귀 머신러닝 함수 \n",
    "\n",
    "def ML(model, param_grid):\n",
    "\n",
    "    # Grid search function\n",
    "    def Grid_set_(model, param_grid):\n",
    "        start_time = time.time() \n",
    "        grid_search = GridSearchCV(model, param_grid, cv=KFold(n_splits=10), scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        end_time = time.time() \n",
    "        grid_search_time = end_time - start_time * 1000 \n",
    "        print(f\"Grid Search Time: {grid_search_time:.2f} seconds\")\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        cv_scores = -cross_val_score(best_model, X_train, y_train, cv=KFold(n_splits=10), scoring='neg_mean_squared_error')\n",
    "        print(f\"***** {model.__class__.__name__} *****\")\n",
    "        print(f\"Cross-validation MSE (Training): {cv_scores.mean():.2f}\")\n",
    "        print(f'Grid Search Best Score (negative MSE): {grid_search.best_score_:.2f}')\n",
    "        return best_model, grid_search, cv_scores\n",
    "\n",
    "    # Evaluate model (common for train, val, test)\n",
    "    def evaluate_model(best_model, X, y):\n",
    "        y_pred = best_model.predict(X)\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "        return mse, mae, r2, y_pred\n",
    "        \n",
    "    # Train data evaluation\n",
    "    def Train_set(best_model, X_train, y_train):\n",
    "        start_time = time.time() \n",
    "        results = evaluate_model(best_model, X_train, y_train)\n",
    "        end_time = time.time() \n",
    "        train_time = end_time - start_time * 1000\n",
    "        print(f\"Training Time: {train_time:.2f} seconds\")\n",
    "        return results\n",
    "\n",
    "    # Validation data evaluation\n",
    "    def Validation_set(best_model, X_val, y_val):\n",
    "        start_time = time.time() \n",
    "        results = evaluate_model(best_model, X_val, y_val)\n",
    "        end_time = time.time() \n",
    "        validation_time = end_time - start_time * 1000\n",
    "        print(f\"Validation Time: {validation_time:.2f} seconds\")\n",
    "        return results\n",
    "\n",
    "    # Test data evaluation\n",
    "    def Test_set(best_model, X_test, y_test):\n",
    "        start_time = time.time()  \n",
    "        results = evaluate_model(best_model, X_test, y_test)\n",
    "        end_time = time.time() \n",
    "        test_time = end_time - start_time  * 1000\n",
    "        print(f\"Test Time: {test_time:.2f} seconds\")\n",
    "        return results\n",
    "\n",
    "\n",
    "    # Bar graph data\n",
    "    def Bar_Source():\n",
    "        train_stats = [{\n",
    "            'MSE_train': mse_train,\n",
    "            'MAE_train': mae_train,\n",
    "            'R2_train': r2_train,\n",
    "            'Model': model.__class__.__name__\n",
    "        }]\n",
    "        train_score.append(train_stats)\n",
    "\n",
    "        val_stats = [{\n",
    "            'MSE_val': mse_val,\n",
    "            'MAE_val': mae_val,\n",
    "            'R2_val': r2_val,\n",
    "            'Model': model.__class__.__name__\n",
    "        }]\n",
    "        val_score.append(val_stats)\n",
    "\n",
    "        test_stats = [{\n",
    "            'MSE_test': mse_test,\n",
    "            'MAE_test': mae_test,\n",
    "            'R2_test': r2_test,\n",
    "            'Model': model.__class__.__name__\n",
    "        }]\n",
    "        test_score.append(test_stats)\n",
    "\n",
    "        representative_stats = [{\n",
    "            'CV_score': cv_scores.mean(),\n",
    "            'Best_score': grid_search.best_score_,\n",
    "            'Model': model.__class__.__name__\n",
    "        }]\n",
    "        representative_score.append(representative_stats)\n",
    "\n",
    "    # Final results\n",
    "    def Result_():\n",
    "        results = {\n",
    "            'Training': {\n",
    "                'mse': mse_train,\n",
    "                'mae': mae_train,\n",
    "                'r2': r2_train,\n",
    "                'predictions': y_train_pred\n",
    "            },\n",
    "            'Validation': {\n",
    "                'mse': mse_val,\n",
    "                'mae': mae_val,\n",
    "                'r2': r2_val,\n",
    "                'predictions': y_val_pred\n",
    "            },\n",
    "            'Test': {\n",
    "                'mse': mse_test,\n",
    "                'mae': mae_test,\n",
    "                'r2': r2_test,\n",
    "                'predictions': y_test_pred\n",
    "            }\n",
    "        }\n",
    "        return results\n",
    "\n",
    "    # Visualization & Output\n",
    "    def output(results):\n",
    "        for stage, result in results.items():\n",
    "            print(f'*****{model.__class__.__name__}*****')\n",
    "            print(f\"Stage: {stage}\")\n",
    "            print(f\"\\nMSE: {result['mse']:.2f}\")\n",
    "            print(f\"\\nMAE: {result['mae']:.2f}\")\n",
    "            print(f\"\\nR^2 Score: {result['r2']:.2f}\")\n",
    "            print('\\n')\n",
    "\n",
    "    best_model, grid_search, cv_scores = Grid_set_(model, param_grid)\n",
    "\n",
    "    # Run evaluations for Train, Validation, Test sets\n",
    "    mse_train, mae_train, r2_train, y_train_pred = Train_set(best_model, X_train, y_train)\n",
    "    mse_val, mae_val, r2_val, y_val_pred = Validation_set(best_model, X_val, y_val)\n",
    "    mse_test, mae_test, r2_test, y_test_pred = Test_set(best_model, X_test, y_test)\n",
    "\n",
    "    # Bar graph source update\n",
    "    Bar_Source()\n",
    "\n",
    "    # Results output\n",
    "    results = Result_()\n",
    "    output(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래프 예시\n",
    "# 그래프 예시 (분류모델)\n",
    "lst = ['train','test','val','representative']\n",
    "\n",
    "for i in lst : \n",
    "    source = [item for sublist in globals()[f'{i}_score'] for item in sublist]\n",
    "    globals()[f'{i}_bar'] = pd.DataFrame(source)\n",
    "\n",
    "print(train_bar)\n",
    "print(test_bar)\n",
    "print(val_bar)\n",
    "print(representative_bar)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "graph_lst = ['train', 'val', 'test']\n",
    "\n",
    "for i in graph_lst:  \n",
    "    fig, axes = plt.subplots(3, 2, figsize=(10, 10))\n",
    "\n",
    "    sns.barplot(data=globals()[f'{i}_bar'].sort_values(f'ROC_AUC_{i}', ascending=False),\n",
    "                x=f'ROC_AUC_{i}', y='Model', color='purple', alpha=0.5, ax=axes[0, 0])\n",
    "    \n",
    "    top_roc_auc = globals()[f'{i}_bar'].sort_values(f'ROC_AUC_{i}', ascending=False).head(1)\n",
    "    sns.barplot(data=top_roc_auc, \n",
    "                x=f'ROC_AUC_{i}', y='Model', color='darkred',alpha=0.7, ax=axes[0, 0])\n",
    "\n",
    "    axes[0, 0].bar_label(axes[0, 0].containers[0], fmt='%.2f', label_type='center', color='white', fontsize=8)    \n",
    "    axes[0, 0].set_title('ROC_AUC')\n",
    "    axes[0, 0].set_ylabel('')\n",
    "    axes[0, 0].set_xlabel('')\n",
    "\n",
    "    sns.barplot(data=globals()[f'{i}_bar'].sort_values(f'PR_AUC_{i}', ascending=False),\n",
    "                x=f'PR_AUC_{i}', y='Model', color='purple', alpha=0.5, ax=axes[0, 1])\n",
    "    \n",
    "    top_pr_auc = globals()[f'{i}_bar'].sort_values(f'PR_AUC_{i}', ascending=False).head(1)\n",
    "    sns.barplot(data=top_pr_auc, \n",
    "                x=f'PR_AUC_{i}', y='Model', color='darkred',alpha=0.7, ax=axes[0, 1])\n",
    "    \n",
    "    \n",
    "    axes[0, 1].bar_label(axes[0, 1].containers[0], fmt='%.2f', label_type='center', color='white', fontsize=8)    \n",
    "    axes[0, 1].set_title('PR_AUC')\n",
    "    axes[0, 1].set_ylabel('')\n",
    "    axes[0, 1].set_xlabel('')\n",
    "\n",
    "    sns.barplot(data=globals()[f'{i}_bar'].sort_values(f'Recall_{i}', ascending=False),\n",
    "                x=f'Recall_{i}', y='Model', color='purple', alpha=0.5, ax=axes[1, 0])\n",
    "    \n",
    "    top_recall = globals()[f'{i}_bar'].sort_values(f'Recall_{i}', ascending=False).head(1)\n",
    "    sns.barplot(data=top_recall, \n",
    "                x=f'Recall_{i}', y='Model', color='darkred',alpha=0.7, ax=axes[1, 0])\n",
    "    \n",
    "    axes[1, 0].bar_label(axes[1, 0].containers[0], fmt='%.2f', label_type='center', color='white', fontsize=8)    \n",
    "    axes[1, 0].set_title('Recall Score_mean')\n",
    "    axes[1, 0].set_ylabel('')\n",
    "    axes[1, 0].set_xlabel('')\n",
    "\n",
    "    sns.barplot(data=globals()[f'{i}_bar'].sort_values(f'Precision_{i}', ascending=False),\n",
    "                x=f'Precision_{i}', y='Model', color='purple', alpha=0.5, ax=axes[1, 1])\n",
    "    \n",
    "    top_pre = globals()[f'{i}_bar'].sort_values(f'Precision_{i}', ascending=False).head(1)\n",
    "    sns.barplot(data=top_pre, \n",
    "                x=f'Precision_{i}', y='Model', color='darkred',alpha=0.7, ax=axes[1, 1])\n",
    "    \n",
    "    axes[1, 1].bar_label(axes[1, 1].containers[0], fmt='%.2f', label_type='center', color='white', fontsize=8)    \n",
    "    axes[1, 1].set_title('Precision Score_mean')\n",
    "    axes[1, 1].set_ylabel('')\n",
    "    axes[1, 1].set_xlabel('')\n",
    "\n",
    "    sns.barplot(data=globals()[f'{i}_bar'].sort_values(f'F1-score_{i}', ascending=False),\n",
    "                x=f'F1-score_{i}', y='Model', color='purple', alpha=0.5, ax=axes[2, 0])\n",
    "    \n",
    "    top_f1 = globals()[f'{i}_bar'].sort_values(f'F1-score_{i}', ascending=False).head(1)\n",
    "    sns.barplot(data=top_f1, \n",
    "                x=f'F1-score_{i}', y='Model', color='darkred',alpha=0.7, ax=axes[2, 0])\n",
    "    axes[2, 0].bar_label(axes[2, 0].containers[0], fmt='%.2f', label_type='center', color='white', fontsize=8)    \n",
    "    axes[2, 0].set_title('F1-score')\n",
    "    axes[2, 0].set_ylabel('')\n",
    "    axes[2, 0].set_xlabel('')\n",
    "\n",
    "    fig.delaxes(axes[2, 1])\n",
    "\n",
    "    fig.suptitle(f'{i.capitalize()} - Data Score')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "sns.barplot(data=representative_bar.sort_values('CV_score', ascending=False),\n",
    "            x='CV_score', y='Model', color='purple', alpha=0.5, ax=axes[0])\n",
    "\n",
    "sns.barplot(data =representative_bar.sort_values('CV_score',ascending=False).head(1), \n",
    "            x = 'CV_score', y= 'Model', color = 'darkred', alpha = .7, ax = axes[0])\n",
    "\n",
    "axes[0].bar_label(axes[0].containers[0], fmt='%.2f', label_type = 'center', color = 'white', fontsize = 8)\n",
    "axes[0].set_title('CV_accuracy')\n",
    "axes[0].set_ylabel('')\n",
    "axes[0].set_xlabel('')\n",
    "\n",
    "sns.barplot(data=representative_bar.sort_values('Best_score', ascending=False),\n",
    "            x='Best_score', y='Model', color='purple', alpha=0.5, ax=axes[1])\n",
    "\n",
    "sns.barplot(data=representative_bar.sort_values('Best_score',ascending=False).head(1), \n",
    "            x = 'Best_score', y= 'Model', color = 'darkred', alpha = .7, ax = axes[1])\n",
    "\n",
    "axes[1].bar_label(axes[1].containers[0], fmt='%.2f', label_type = 'center', color = 'white', fontsize = 8)\n",
    "axes[1].set_title('Grid Search_Best_score')\n",
    "axes[1].set_ylabel('')\n",
    "axes[1].set_xlabel('')\n",
    "fig.suptitle('Representative - Data Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# melt 활용한 그래프 예시 (분류)\n",
    "----------------------------------------------------------------\n",
    "train_bar['Dataset'] = 'Train'\n",
    "val_bar['Dataset'] = 'Validation'\n",
    "test_bar['Dataset'] = 'Test'\n",
    "\n",
    "combined_bar = pd.concat([train_bar, val_bar, test_bar], ignore_index=True)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "sns.barplot(data=combined_bar.melt(id_vars=['Model', 'Dataset'], value_vars=['ROC_AUC_train', 'ROC_AUC_val', 'ROC_AUC_test']),\n",
    "            x='value', y='Model', hue='Dataset', palette='inferno', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('ROC AUC')\n",
    "axes[0, 0].set_xlabel('')\n",
    "axes[0, 0].set_ylabel('')\n",
    "axes[0, 0].legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "\n",
    "sns.barplot(data=combined_bar.melt(id_vars=['Model', 'Dataset'], value_vars=['PR_AUC_train', 'PR_AUC_val', 'PR_AUC_test']),\n",
    "            x='value', y='Model', hue='Dataset', palette='inferno', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('PR AUC')\n",
    "axes[0, 1].set_xlabel('')\n",
    "axes[0, 1].set_ylabel('')\n",
    "axes[0, 1].legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "\n",
    "sns.barplot(data=combined_bar.melt(id_vars=['Model', 'Dataset'], value_vars=['Recall_train', 'Recall_val', 'Recall_test']),\n",
    "            x='value', y='Model', hue='Dataset', palette='inferno', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Recall')\n",
    "axes[1, 0].set_xlabel('')\n",
    "axes[1, 0].set_ylabel('')\n",
    "axes[1, 0].legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "\n",
    "sns.barplot(data=combined_bar.melt(id_vars=['Model', 'Dataset'], value_vars=['Precision_train', 'Precision_val', 'Precision_test']),\n",
    "            x='value', y='Model', hue='Dataset', palette='inferno', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Precision')\n",
    "axes[1, 1].set_xlabel('')\n",
    "axes[1, 1].set_ylabel('')\n",
    "axes[1, 1].legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "\n",
    "sns.barplot(data=combined_bar.melt(id_vars=['Model', 'Dataset'], value_vars=['F1-score_train', 'F1-score_val', 'F1-score_test']),\n",
    "            x='value', y='Model', hue='Dataset', palette='inferno', ax=axes[2, 0])\n",
    "axes[2, 0].set_title('F1 Score')\n",
    "axes[2, 0].set_xlabel('')\n",
    "axes[2, 0].set_ylabel('')\n",
    "axes[2, 0].legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "\n",
    "fig.delaxes(axes[2, 1])  \n",
    "\n",
    "fig.suptitle('Total-Data')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(data=combined_bar.melt(id_vars=['Model','Dataset'], value_vars=['Accuracy_train','Accuracy_val','Accuracy_test']),\n",
    "            x = 'value', y='Model', hue='Dataset',palette='magma')\n",
    "plt.title('Model_Fitting_Check (Accuracy)')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.legend(loc = 'upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하이퍼파라미터를 알아봅시다.\n",
    "\n",
    "# model = LogisticRegression(\n",
    "    penalty='l2',           # 규제 종류 ('l1', 'l2', 'elasticnet' 또는 'none')\n",
    "    dual=False,             # 듀얼 폼 사용 여부 (샘플 수 > 특성 수인 경우만 사용)\n",
    "    tol=1e-4,               # 허용 오차\n",
    "    C=1.0,                  # 규제 강도 (작을수록 규제가 강해짐)\n",
    "    solver='lbfgs',         # 최적화 알고리즘 ('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga')\n",
    "    max_iter=100,           # 최대 반복 횟수\n",
    "    random_state=42,        # 난수 시드\n",
    "    class_weight=None,      # 클래스 가중치 ('balanced' 또는 None)\n",
    "    multi_class='auto'      # 다중 클래스 문제 해결 방식 ('auto', 'ovr', 'multinomial')\n",
    ")\n",
    "\n",
    "# model = DecisionTreeClassifier(\n",
    "    criterion='gini',        # 분할 품질 기준 ('gini' 또는 'entropy')\n",
    "    splitter='best',         # 분할 전략 ('best' 또는 'random')\n",
    "    max_depth=None,          # 트리의 최대 깊이\n",
    "    min_samples_split=2,     # 분할을 위한 최소 샘플 수\n",
    "    min_samples_leaf=1,      # 리프 노드가 될 수 있는 최소 샘플 수\n",
    "    max_features=None,       # 분할 시 고려할 최대 특성 수\n",
    "    random_state=42,         # 난수 시드\n",
    "    max_leaf_nodes=None      # 최대 리프 노드 수\n",
    ")\n",
    "\n",
    "# model = RandomForestClassifier(\n",
    "    n_estimators=100,        # 트리 개수\n",
    "    criterion='gini',        # 분할 품질 기준 ('gini' 또는 'entropy')\n",
    "    max_depth=None,          # 트리의 최대 깊이\n",
    "    min_samples_split=2,     # 분할을 위한 최소 샘플 수\n",
    "    min_samples_leaf=1,      # 리프 노드가 될 수 있는 최소 샘플 수\n",
    "    max_features='auto',     # 분할 시 고려할 최대 특성 수 ('auto', 'sqrt', 'log2')\n",
    "    bootstrap=True,          # 부트스트래핑 여부\n",
    "    random_state=42,         # 난수 시드\n",
    "    n_jobs=-1                # 병렬 처리할 CPU 코어 수 (-1은 모든 코어 사용)\n",
    ")\n",
    "\n",
    "# model = GradientBoostingClassifier(\n",
    "    loss='deviance',         # 손실 함수 ('deviance' 또는 'exponential')\n",
    "    learning_rate=0.1,       # 학습 속도\n",
    "    n_estimators=100,        # 부스팅 단계 개수\n",
    "    subsample=1.0,           # 각 트리마다 샘플링할 비율\n",
    "    criterion='friedman_mse',# 분할 품질 기준 ('friedman_mse', 'mse', 'mae')\n",
    "    max_depth=3,             # 트리의 최대 깊이\n",
    "    min_samples_split=2,     # 분할을 위한 최소 샘플 수\n",
    "    random_state=42,         # 난수 시드\n",
    "    max_features=None        # 분할 시 고려할 최대 특성 수\n",
    ")\n",
    "\n",
    "# model = KNeighborsClassifier(\n",
    "    n_neighbors=5,           # 이웃할 샘플 수\n",
    "    weights='uniform',       # 가중치 ('uniform' 또는 'distance')\n",
    "    algorithm='auto',        # 알고리즘 ('auto', 'ball_tree', 'kd_tree', 'brute')\n",
    "    leaf_size=30,            # 트리 기반 알고리즘에서 사용할 리프 크기\n",
    "    p=2,                     # Minkowski 거리에서 사용할 차수\n",
    "    metric='minkowski',      # 거리 측정 기준\n",
    "    n_jobs=-1                # 병렬 처리할 CPU 코어 수 (-1은 모든 코어 사용)\n",
    ")\n",
    "\n",
    "# model = GaussianNB(\n",
    "    var_smoothing=1e-9       # 각 클래스의 가우시안 분포에 추가하는 분산 값\n",
    ")\n",
    "\n",
    "# model = xgb.XGBClassifier(\n",
    "    booster='gbtree',        # 부스터 종류 ('gbtree', 'gblinear', 'dart')\n",
    "    learning_rate=0.1,       # 학습 속도\n",
    "    n_estimators=100,        # 부스팅 단계 개수\n",
    "    max_depth=3,             # 트리의 최대 깊이\n",
    "    subsample=1.0,           # 각 트리마다 샘플링할 비율\n",
    "    colsample_bytree=1.0,    # 각 트리에서 사용할 특성 비율\n",
    "    gamma=0,                 # 리프 노드 추가에 대한 최소 손실 감소\n",
    "    reg_alpha=0,             # L1 규제 강도\n",
    "    reg_lambda=1,            # L2 규제 강도\n",
    "    random_state=42          # 난수 시드\n",
    ")\n",
    "\n",
    "# model = lgb.LGBMClassifier(\n",
    "    boosting_type='gbdt',    # 부스팅 유형 ('gbdt', 'dart', 'goss', 'rf')\n",
    "    num_leaves=31,           # 트리의 최대 리프 노드 개수\n",
    "    learning_rate=0.1,       # 학습 속도\n",
    "    n_estimators=100,        # 부스팅 단계 개수\n",
    "    max_depth=-1,            # 트리의 최대 깊이 (-1은 무제한)\n",
    "    subsample=1.0,           # 샘플링할 비율\n",
    "    colsample_bytree=1.0,    # 각 트리에서 사용할 특성 비율\n",
    "    reg_alpha=0.0,           # L1 규제 강도\n",
    "    reg_lambda=0.0,          # L2 규제 강도\n",
    "    random_state=42          # 난수 시드\n",
    ")\n",
    "\n",
    "# model = ExtraTreeClassifier(\n",
    "    criterion='gini',        # 분할 품질 기준 ('gini' 또는 'entropy')\n",
    "    splitter='random',       # 분할 전략 ('best' 또는 'random')\n",
    "    max_depth=None,          # 트리의 최대 깊이\n",
    "    min_samples_split=2,     # 분할을 위한 최소 샘플 수\n",
    "    min_samples_leaf=1,      # 리프 노드가 될 수 있는 최소 샘플 수\n",
    "    random_state=42          # 난수 시드\n",
    ")\n",
    "\n",
    "# model = SVC(\n",
    "    C=1.0,                   # 규제 강도 (C가 작을수록 규제가 강해짐)\n",
    "    kernel='rbf',            # 커널 종류 ('linear', 'poly', 'rbf', 'sigmoid')\n",
    "    degree=3,                # 다항 커널에서의 차수\n",
    "    gamma='scale',           # 커널 계수 ('scale', 'auto')\n",
    "    probability=True,        # 확률 추정 여부\n",
    "    random_state=42          # 난수 시드\n",
    ")\n",
    "\n",
    "# model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,), # 은닉층 크기\n",
    "    activation='relu',         # 활성화 함수 ('identity', 'logistic', 'tanh', 'relu')\n",
    "    solver='adam',             # 최적화 알고리즘 ('lbfgs', 'sgd', 'adam')\n",
    "    alpha=0.0001,              # L2 규제 강도\n",
    "    batch_size='auto',         # 미니 배치 크기\n",
    "    learning_rate='constant',  # 학습률 스케줄 ('constant', 'invscaling', 'adaptive')\n",
    "    max_iter=200,              # 최대 반복 횟수\n",
    "    random_state=42            # 난수 시드\n",
    ")\n",
    "\n",
    "# model = LinearDiscriminantAnalysis(\n",
    "    solver='svd',             # 알고리즘 ('svd', 'lsqr', 'eigen')\n",
    "    shrinkage=None,           # Shrinkage (None, 'auto', float)\n",
    "    priors=None,              # 클래스별 사전 확률\n",
    "    n_components=None         # 축소할 차원 수\n",
    ")\n",
    "\n",
    "# model = QuadraticDiscriminantAnalysis(\n",
    "    reg_param=0.0,            # 정규화 파라미터\n",
    "    priors=None,              # 클래스별 사전 확률\n",
    "    tol=1e-4                  # 허용 오차\n",
    ")\n",
    "\n",
    "# model = AdaBoostClassifier(\n",
    "    n_estimators=50,       # 사용할 약한 학습기의 개수\n",
    "    learning_rate=1.0,     # 학습 속도\n",
    "    algorithm='SAMME.R',   # 'SAMME' 또는 'SAMME.R' 알고리즘\n",
    "    random_state=42        # 난수 시드\n",
    ")\n",
    "\n",
    "# model = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(), # 기본 학습기\n",
    "    n_estimators=10,        # 기본 학습기의 개수\n",
    "    max_samples=1.0,        # 각 기본 학습기가 훈련에 사용할 샘플 비율\n",
    "    max_features=1.0,       # 각 기본 학습기가 훈련에 사용할 특성 비율\n",
    "    bootstrap=True,         # 부트스트래핑 여부\n",
    "    random_state=42         # 난수 시드\n",
    ")\n",
    "\n",
    "# model = GaussianProcessClassifier(\n",
    "    kernel=RBF(),           # 사용되는 커널\n",
    "    random_state=42         # 난수 시드\n",
    ")\n",
    "\n",
    "# model = LogisticRegressionCV(\n",
    "    Cs=10,                  # 시도할 C 값의 개수\n",
    "    cv=5,                   # 교차 검증 폴드 수\n",
    "    penalty='l2',            # 패널티 종류 ('l1', 'l2' 가능)\n",
    "    solver='lbfgs',          # 최적화 알고리즘\n",
    "    max_iter=100,            # 최대 반복 횟수\n",
    "    random_state=42          # 난수 시드\n",
    ")\n",
    "\n",
    "# model = PassiveAggressiveClassifier(\n",
    "    C=1.0,                  # 규제 강도 (C가 클수록 규제가 약해짐)\n",
    "    max_iter=1000,           # 최대 반복 횟수\n",
    "    tol=1e-3,                # 허용 오차\n",
    "    random_state=42          # 난수 시드\n",
    ")\n",
    "\n",
    "# model = RidgeClassifierCV(\n",
    "    alphas=[0.1, 1.0, 10.0], # 시도할 alpha 값의 리스트\n",
    "    cv=5                      # 교차 검증 폴드 수\n",
    ")\n",
    "\n",
    "# model = SGDClassifier(\n",
    "    loss='hinge',             # 손실 함수 ('hinge' for SVM, 'log' for logistic regression)\n",
    "    penalty='l2',             # 규제 ('l2', 'l1', 'elasticnet' 가능)\n",
    "    max_iter=1000,            # 최대 반복 횟수\n",
    "    tol=1e-3,                 # 허용 오차\n",
    "    random_state=42           # 난수 시드\n",
    ")\n",
    "\n",
    "# model = Perceptron(\n",
    "    max_iter=1000,            # 최대 반복 횟수\n",
    "    tol=1e-3,                 # 허용 오차\n",
    "    penalty=None,             # 규제 ('l2', 'l1', 'elasticnet' 가능)\n",
    "    random_state=42           # 난수 시드\n",
    ")\n",
    "\n",
    "# model = BernoulliNB(\n",
    "    alpha=1.0,                # 라플라스 스무딩 파라미터\n",
    "    binarize=0.0              # 이진화 임계값\n",
    ")\n",
    "\n",
    "# model = NuSVC(\n",
    "    nu=0.5,                  # 규제 파라미터 (0과 1 사이)\n",
    "    kernel='rbf',            # 커널 종류 ('linear', 'poly', 'rbf', 'sigmoid' 가능)\n",
    "    probability=True,        # 확률 추정 여부\n",
    "    random_state=42          # 난수 시드\n",
    ")\n",
    "\n",
    "# model = LinearSVC(\n",
    "    C=1.0,                   # 규제 강도\n",
    "    max_iter=1000,            # 최대 반복 횟수\n",
    "    tol=1e-3,                 # 허용 오차\n",
    "    random_state=42           # 난수 시드\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기법 모듈\n",
    "# SKlearn Module\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,cross_validate,cross_val_score,RandomizedSearchCV,StratifiedKFold,ShuffleSplit,KFold,LeaveOneOut\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier,BaggingClassifier\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve,accuracy_score,f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV,PassiveAggressiveClassifier,RidgeClassifierCV,SGDClassifier,Perceptron\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier,ExtraTreeClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.svm import SVC,NuSVC,LinearSVC\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "from sklearn.gaussian_process.kernels import RBF\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
